[{"body":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy and track CF Deployments, consuming directly also BOSH Releases, among Kubernetes Native components.\nWhere should I go next?  Getting Started: Get started with KubeCF Tutorials: Check out some tutorials!  ","excerpt":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy …","ref":"https://kubecf.suse.dev/docs/overview/","title":"Overview"},{"body":" The bazel target //testing/acceptance_tests starts a run of the Cloud Foundry Acceptance Tests.\nSee also the entire set of available tests.\nLimiting test suites To limit the test groups to run, set the BOSH property acceptance_tests.include as documented. To do so, adjust the properties key in values.yaml to specify the groups desired. For example:\nproperties:acceptance-tests:acceptance-tests:acceptance_tests:include:\u0026#34;+docker,-ssh\u0026#34; Note that this is an example of how to use the second kind of customization feature noted in the main README.\n","excerpt":"The bazel target //testing/acceptance_tests starts a run of the Cloud Foundry Acceptance Tests.\nSee …","ref":"https://kubecf.suse.dev/docs/reference/tests/","title":"Tests"},{"body":" There is not an official release yet, check out our nightlies.\n Prerequisites  A Kubernetes cluster Presence of a default storage class (provisioner). For use with a diego-based kubecf (default), a node OS with XFS support.  For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   Installation KubeCF is packaged as an Helm chart.\nCurrently there isn\u0026rsquo;t any official release.\nNightly builds can be found on the KubeCF public s3 bucket.\nTry it out! ","excerpt":"There is not an official release yet, check out our nightlies.\n Prerequisites  A Kubernetes cluster …","ref":"https://kubecf.suse.dev/docs/getting-started/","title":"Getting Started"},{"body":" Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The Image Modify Kubecf to Use the New Image  Integrating the Release in Kubecf  BPM Operation Files  Testing With Kubecf  Preparing the Release Image BOSH release authors who want to test their development code with the Quarks operator need to build a Docker image from their release. This can be done with fissile. Afterwards, upload the image to a cluster for testing it, e.g. with Kubecf.\nBuilding a Docker Image with Fissile Build the BOSH release first and convert it with fissile.\nTo generate a docker image from the BOSH release, you should use the following subcommand:\nfissile build release-image For more information on how to use the command, please refer to the related documentation. For a real example, see build.sh.\nUploading The Image Depending on your cluster, you will need a way to get the locally built image into the Kubernetes registry.\nWith minikube you can build directly on minikube\u0026rsquo;s Docker. Switch to that docker daemon by running eval $(minikube docker-env), before you build the image with fissile.\nWith kind, you need to use kind load docker-image after building the image, to make it available, i.e.:\nkind load docker-image docker.io/org/nats:0.1-dev Modify Kubecf to Use the New Image Add an operations file to Kubernetes with the new image location. The example below uses NATS as the example for a BOSH release.\nkubectlapply-f-\u0026lt;\u0026lt;EOF---apiVersion:v1kind:ConfigMapmetadata:name:nats-devdata:ops:|-type:replacepath:/releases/name=nats?value:name:natsurl:docker.io/org/natsversion:0.1-devsha1:~EOF Then, when running helm install kubecf, refer to that image:\nhelm install ... --set \u0026#39;operations.custom={nats-dev}\u0026#39; Note: You can also unpack the helm release and modify it directly. There is no need to zip the release again, as helm install scf/ is able to install the unpacked release.\nNote further that the above is an example of how to use the first kind of customization feature noted in the main README.\nIntegrating the Release in Kubecf With Quarks and Kubecf, BOSH releases can largely be used just the same as with a BOSH director. There are a few things Quarks offers, however, to make the adaptation to the Kubernetes environment easier.\nBPM BPM configurations for jobs are parsed from a rendered bpm.yml, as usual. But if need be, it is also possible to override the BPM configuration in the deployment manifest in the quarks field. See the bpm documentation for details on how to configure BPM.\nExample:\ninstance_groups:-name:natsinstances:2jobs:-name:natsproperties:quarks:bpm:processes:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Note: The next section on ops files explains how this can be applied without the need to modify the original deployment manifest using ops files.\nOperation Files ops files can be used to modify arbitrary parts of the deployment manifest before it is applied. To do so, create a file in the directory deploy/helm/scf/assets/operations/instance_groups and it will be automagically applied during installation, courtesy of the bazel machinery.\nThe ops file for the example above could look like this:\n-type:replacepath:/instance_groups/name=nats/jobs/name=nats/properties/quarks?/bpm/processesvalue:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Testing With Kubecf After upload and integration, it is possible to build and deploy Kubecf according to any of the recipes listed by the main README.\n","excerpt":"Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The …","ref":"https://kubecf.suse.dev/docs/tutorials/bosh-integration/","title":"Bosh releases integration"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf locally using:\n Minikube to manage a local Kubernetes cluster. A cf-operator pinned with Bazel. Kubecf built and deployed from the sources in the current checkout.  Minikube Minikube is one of several projects enabling the deployment, management and tear-down of a local Kubernetes cluster.\nThe Kubecf Bazel workspace contains targets to deploy and/or tear-down a Minikube-based cluster. Using these has the advantage of using a specific version of Minikube. On the other side, the reduced variability of the development environment is a disadvantage as well, possibly allowing portability issues to slide through.\n   Operation Command     Deployment bazel run //dev/minikube:start   Tear-down bazel run //dev/minikube:delete    Attention, Dangers Minikube edits the Kubernetes configuration file referenced by the environment variable KUBECONFIG, or ~/.kube/config.\nTo preserve the original configuration either make a backup of the relevant file, or change KUBECONFIG to a different path specific to the intended deployment.\nAdvanced configuration The local Minikube Documentation explains the various environment variables which can be used to configure the resources used by the cluster (CPUs, memory, disk size, etc.) in detail.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same kube cluster Kubecf will be deployed to.\nDeployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down cf-operator:\n   Operation Command     Deployment bazel run //dev/cf_operator:apply   Tear-down bazel run //dev/cf_operator:delete    Kubecf With all the prequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nSystem domain The main configuration to set for kubecf is its system domain. For the Minikube foundation we have to specify it as:\necho \u0026#34;system_domain: $(minikube ip).xip.io\u0026#34; \\  \u0026gt; \u0026#34;$(bazel info workspace)/dev/kubecf/system_domain_values.yaml\u0026#34; Deployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down kubecf from the sources:\n   Operation Command     Deployment bazel run //dev/kubecf:apply   Tear-down bazel run //dev/kubecf:delete    In this default deployment kubecf is launched without Ingress, and uses the Diego scheduler.\nAccess Accessing the cluster from outside of the minikube VM requires ingress to be set up correctly.\nTo access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.$(minikube ip).xip.io\u0026#34; # Copy the admin cluster password. acp=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${acp}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, add a file matching the pattern *values.yaml to the directory dev/kubecf and containing\nfeatures:eirini:enabled:true before deploying kubecf.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/kubecf-scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; \\  --set \u0026#34;controller.service.externalIPs={$(minikube ip)}\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nThe last flag in the command above assigns the external IP of the cluster to the Ingress Controller service.\nConfigure kubecf Place a file matching the pattern *values.yaml into the directory dev/kubecf and containing\nfeatures:ingress:enabled:true","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/tutorials/deploy-minikube/","title":"Deploy KubeCF in Minikube"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/concepts/","title":"Concepts"},{"body":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator enables the deployment of BOSH Releases, especially Cloud Foundry, to Kubernetes.\nIt\u0026rsquo;s implemented as a k8s operator, an active controller component which acts upon custom k8s resources.\n Installation notes Incubation Proposal: Containerizing Cloud Foundry Slack: #quarks-dev on https://slack.cloudfoundry.org Backlog: Pivotal Tracker Docker: https://hub.docker.com/r/cfcontainerization/cf-operator/tags  ","excerpt":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator enables the deployment of BOSH …","ref":"https://kubecf.suse.dev/docs/concepts/operator/","title":"Cloud Foundry Operator"},{"body":"Project Quarks is an incubating effort within the Cloud Foundry Foundation that packages Cloud Foundry Application Runtime as containers instead of virtual machines, enabling easy deployment to Kubernetes.\nThe resulting containerized CFAR provides an identical developer experience to that of BOSH-managed Cloud Foundry installations, requires less infrastructure capacity and delivers an operational experience that is familiar to Kubernetes operators.\n","excerpt":"Project Quarks is an incubating effort within the Cloud Foundry Foundation that packages Cloud …","ref":"https://kubecf.suse.dev/docs/concepts/quarks/","title":"Project Quarks"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/","title":"Core Tasks"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/secrets/","title":"Secret rotation KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/deploy/","title":"Deploy KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/upgrade/","title":"Upgrading KubeCF deployments"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/bosh/","title":"Test BOSH releases"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tutorials/","title":"Tutorials"},{"body":"When running a kubecf instance, all of the job containers in the pods for the instance groups use a docker image. This image provides that job and the associated packages.\nThe names of these docker images are structured like so:\ndocker.io/cfcontainerization/nats:opensuse-42.3-36.g03b4653-30.80-7.0.0_362.g9610e90b-26 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Registry Role Tag  The tag is further structured as:\nopensuse-42.3-36.g03b4653-30.80-7.0.0_362.g9610e90b-26 ~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~ Stemcell OS Stemcell version Role version  The various components above come from a number of places in the Chart, chart values, deployment manifest, etc. When an element can be provided by multiple places, the first place (in the order given below) with a defined value (i.e. not nil) is used.\n   Element Origin     Role instance_groups.[].name   Registry releases.(role).url    releases.defaults.url   Role version releases.(role).version    releases.defaults.version   Stemcell OS releases.(role).stemcell.os    releases.defaults.stemcell.os   Stemcell version releases.(role).stemcell.version    releases.defaults.stemcell.version    Attention: The stemcell information put into the Chart / manifest has to match the stemcell baked into the docker image by the CI image builder at the time of building.\nThis information is easiest to extract from the tag for the used docker image.\nSome, but not all, of this information can also be extracted from the labels of the used docker image. For image foo, invoke:\ndocker inspect -f '{{ index .Config.Labels \u0026quot;stemcell-version\u0026quot; }}' foo docker inspect -f '{{ index .Config.Labels \u0026quot;stemcell-flavor\u0026quot; }}' foo  The flavor plus the part of the version up to the first dash character (-) provides the stemcell OS, and the remainder of the version provides a prefix for the stemcell version.\nFor example, the version and flavor strings\n42.3-36.g03b4653-30.80 opensuse  yield opensuse-42.3 and 36.g03b4653-30.80 for os and version prefix. Below, the tag structure again, with the labeling information added:\nstemcell-flavor | stemcell-version ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~ opensuse-42.3-36.g03b4653-30.80-7.0.0_362.g9610e90b-26 ~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~ Stemcell OS Stemcell version Role version  ","excerpt":"When running a kubecf instance, all of the job containers in the pods for the instance groups use a …","ref":"https://kubecf.suse.dev/docs/reference/image-naming/","title":"Image naming"},{"body":"Here are grouped all the docs which refers to internals or specific code areas of KubeCF\n","excerpt":"Here are grouped all the docs which refers to internals or specific code areas of KubeCF","ref":"https://kubecf.suse.dev/docs/reference/","title":"Reference"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nIt provides a basic overview of various aspects of the project below, and uses these overviews as the launching points to other documents which go deeper into the details of each aspect.\n Table of Contents (Aspects)  Deployment Pull Requests Source Organization Docker Images Linting Patching BOSH Development Workflow  Deployment Kubecf is built on top of a number of technologies, namely Kubernetes, Helm (charts), and the cf-operator.\nFor all these we have multiple choices for installing them, and various interactions between the choices influence the details of the commands to use.\nInstead of trying to document all the possibilities and all their interactions at once, supporting documents will describe specific combinations of choices in detail, from the bottom up.\n   Document Description     Local Minikube Minikube/Bazel + Operator/Bazel + Kubecf/Bazel   General Kube Any Kube + Operator/Helm + Kubecf/Helm    Pull Requests The general work flow for pull requests contributing bug fixes, features, etc. is:\n Branch or Fork the suse/kubecf repository, depending on permissions.\n Implement the bug fix, feature, etc. on that branch/fork.\n Submit a pull request based on the branch/fork through the github web interface, against the master branch.\n Developers will review the content of the pull request, asking questions, requesting changes, and generally discussing the submission with the submitter and among themselves.\n After all issues with the request are resolved, and CI has passed, a developer will merge it into master.\n Note that it may be necessary to rebase the branch/fork to resolve any conflicts due to other PRs getting merging while the PR is under discussion.\nSuch a rebase will be a change request from the developers to the contributor, on the assumption that the contributor is best suited to resolving the conflicts.\n  Docker Images The docker images used by kubecf to run jobs in container use a moderately complex naming scheme.\nThis scheme is explained in a separate document: The Naming Of Docker Images in kubecf.\nLinting Currently, 3 linters are available:\n dev/linters/shellcheck.sh dev/linters/yamllint.sh dev/linters/helmlint.sh  Invoke these linters as\ndev/linters/shellcheck.sh dev/linters/yamllint.sh dev/linters/helmlint.sh to run shellcheck on all .sh files found in the entire checkout, or yamllint on all .yaml or .yml files respectively, and report any issues found. The last option runs helm lint (without --strict) on the generated helm chart.\nPatching Background The main goal of the CF operator is to take a BOSH deployment manifest, deploy it, and have it run as-is.\nNaturally, in practice, this goal is not quite reached yet, requiring patching of the deployment manifest in question, and/or the involved releases, at various points of the deployment process. The reason behind a patch is generally fixing a problem, whether it be from the translation into the kube environment, an issue with an underlying component, or something else.\nThen, there are features, given the user of the helm chart wrapped around the deployment manifest the ability to easily toggle various preset configurations, for example the use of eirini instead of diego as the application scheduler.\nFeatures A feature of kubecf is usually implemented using a combination of Helm templating and BOSH ops files.\nThe helm templating is used to translate the properties in the chart\u0026rsquo;s values.yaml to the actual actions to take, by including/excluding chart elements, often the BOSH ops files containing the structured patches modifying the deployment itself (changing properties, adding/removing releases, (de)activating jobs, etc.)\nThe helm templating is applied when the kubecf chart is deployed.\nThe ops files are then applied by the operator, transforming the base manifest from the chart into the final manifest to deploy.\nCustomization Kubecf provides two mechanisms for customization during development (and maybe by operators ?):\n The property .Values.operations.custom of the chart is a list of names for kube configmaps containing the texts of the ops files to apply beyond the ops files from the chart itself.\nNote that we are talking here about a yaml structure whose data.ops property is a text block holding the yaml structure of an ops file.\nThere is no tooling to help the writer with the ensuing quoting hell.\nNote further that the resulting config maps have to be applied, i.e. uploaded into the kube cluster before deploying the kubecf helm chart with its modified values.yaml.\nFor example, kubectl apply the object below\n---apiVersion:v1kind:ConfigMapmetadata:name:configmap_namedata:ops:|-some_random_ops and then use\noperations:custom:-configmap_name  in the values.yaml (or an equivalent --set option) as part of a kubecf deployment to include that ops file in the deployment.\nThe BOSH Development Workflow is an example of its use.\n The second mechanism allows the specification of any custom BOSH property for any instancegroup and job therein.\nJust specifying\nproperties:instance-group-name:job-name:some-property:some-value  in the values.yaml for the kubecf chart causes the chart to generate and use an ops file which applies the assignment of some-value to some-property to the specified instance group and job during deployment.\nAn example of its use in Kubecf is limiting the set of test suites executed by the CF acceptance tests.\nBoth forms of customization assume a great deal of familiarity on the part of the developer and/or operator with the BOSH releases, instance groups and jobs underlying the CF deployment manifest, i.e. which properties exist, what changes to them mean and how they affect the system.\nPatches In SCF v2, the predecessor to kubecf, the patches scripts enabled developers and maintainers to apply general patches to the sources of a job (i.e. configuration templates, script sources, etc.) before that job was rendered and then executed. At the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nPre render scripts are the equivalent feature of the CF operator.\nKubecf makes use of this feature to fix a number of issues in the deployment. The relevant patch scripts are found under the directory bosh/releases/pre_render_scripts.\nWhen following the directory structure explained by the README, the bazel machinery for generating the kubecf helm chart will automatically convert these scripts into the proper ops files for use by the CF operator.\nAttention All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\n","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":" KubeCF is under active development. Note there might be discrepancies between the docs and latest releases.\n ","excerpt":" KubeCF is under active development. Note there might be discrepancies between the docs and latest …","ref":"https://kubecf.suse.dev/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/blog/news/","title":"News About KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/blog/releases/","title":"New Releases"},{"body":" This is a typical blog post that includes images.\nThe front matter specifies the date of the blog post, its title, a short description that will be displayed on the blog landing page, and its author.\nIncluding images Here\u0026rsquo;s an image (featured-sunset-get.png) that includes a byline and a caption.\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\n  The front matter of this post specifies properties to be assigned to all image resources:\nresources: - src: \u0026quot;**.{png,jpg}\u0026quot; title: \u0026quot;Image #:counter\u0026quot; params: byline: \u0026quot;Photo: Riona MacNamara / CC-BY-CA\u0026quot;  To include the image in a page, specify its details like this:\n Fetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\n   The image will be rendered at the size and byline specified in the front matter.\n","excerpt":"This is a typical blog post that includes images.\nThe front matter specifies the date of the blog …","ref":"https://kubecf.suse.dev/blog/2018/10/06/easy-documentation-with-docsy/","title":"Easy documentation with Docsy"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/blog/2018/01/04/another-great-release/","title":"Another Great Release"},{"body":" The scripts in directory dev/cf_deployment/bump are used when bumping the version of the cf-deployment manifest.\nRequirements  yq  Scripts  set_release_urls.sh:\nTo be run after updating the cf-deployment version to use and regenerating assets/cf-deployment.yml, this script regenerates the assets/operations/set_release_urls.yaml ops file which replaces BOSH release locations with references to the equivalent docker images.\n  ","excerpt":" The scripts in directory dev/cf_deployment/bump are used when bumping the version of the …","ref":"https://kubecf.suse.dev/docs/reference/layout/bump/","title":"CF Bump scripts"},{"body":"The cf_cli.sh script contained in directory dev/cf_cli is a temporary helper script that creates a pod with the cf-cli configured and ready to talk to the Kubecf in the cluster.\n","excerpt":"The cf_cli.sh script contained in directory dev/cf_cli is a temporary helper script that creates a …","ref":"https://kubecf.suse.dev/docs/reference/layout/cfcli/","title":"CF CLI script"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf using:\n A generic kubernetes cluster. A released cf-operator helm chart. A released kubecf helm chart.  Kubernetes In contrast to other recipes, we are not set on using a local cluster. Any Kubernetes cluster will do, assuming that the following requirements are met:\n Presence of a default storage class (provisioner).\n For use with a diego-based kubecf (default), a node OS with XFS support.\n For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   This can be any of, but not restricted to:\n GKE (Notes) AKS EKS  Note that how to deploy and tear-down such a cluster is outside of the scope of this recipe.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same Kubernetes cluster that Kubecf will be deployed to.\nHere we are not using development-specific dependencies like bazel, but only generic tools, i.e. kubectl and helm.\nInstalling and configuring Helm is the same regardless of the chosen foundation, and assuming that the cluster does not come with Helm Tiller pre-installed.\nDeployment and Tear-down helm install --name cf-operator \\  --namespace cfo \\  --set \u0026#34;global.operator.watchNamespace=kubecf\u0026#34; \\  https://s3.amazonaws.com/cf-operators/helm-charts/cf-operator-v0.4.2-147.gb88e4296.tgz In the example above, version 0.4.1 of the operator was used. Look into the cf_operator section of the top-level def.bzl file to find the version of the operator validated against the current kubecf master.\nNote: \u0026gt; The above helm install will generate many controllers spread over multiple pods inside the cfo namespace. \u0026gt; Most of these controllers run inside the cf-operator pod. \u0026gt; \u0026gt; The global.operator.watchNamespace=kubecf path tells the controllers to watch for CRD´s instances into the kubecf namespace. \u0026gt; \u0026gt; The cf-operator helm chart will generate the kubecf namespace during installation, and eventually one of the controllers will use a webhook to label this namespace with the cf-operator-ns key. \u0026gt; \u0026gt; If the kubecf namespace is deleted, but the operators are still running, they will no longer know which namespace to watch. This can lead to problems, so make sure you also delete the pods inside the cfo namespace, after deleting the kubecf namespace.\nNote how the namespace the operator is installed into (cfo) differs from the namespace the operator is watching for deployments (kubecf).\nThis form of deployment enables restarting the operator because it is not affected by webhooks. It further enables the deletion of the Kubecf deployment namespace to start from scratch, without redeploying the operator itself.\nTear-down is done with a standard helm delete ... command.\nKubecf With all the prerequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nThis again uses helm and a released helm chart.\nDeployment and Tear-down helm install --name kubecf \\  --namespace kubecf \\  https://scf-v3.s3.amazonaws.com/scf-3.0.0-82165ef3.tgz \\  --set \u0026#34;system_domain=kubecf.suse.dev\u0026#34; In this default deployment, kubecf is launched without Ingress, and it uses the Diego scheduler.\nTear-down is done with a standard helm delete ... command.\nAccess To access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.\u0026lt;domain\u0026gt;\u0026#34; # Copy the admin cluster password. admin_pass=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${admin_pass}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, use the option --set features.eirini.enabled=true when deploying kubecf from its chart.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/kubecf-scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nConfigure kubecf Use the Helm option --set features.ingress.enabled=true when deploying kubecf.\nExternal Database By default, kubecf includes a single-availability database provided by the cf-mysql-release. Kubecf also exposes a way to use an external database via the Helm property features.external_database. Check the values.yaml for more details.\nFor local development with an external database, the bazel run //dev/external_database:deploy_mysql command will bring a mysql database up and running ready to be consumed by kubecf.\nAn example for the additional values to be provided to //dev/kubecf:apply:\nfeatures:external_database:enabled:truetype:mysqlhost:kubecf-mysql.kubecf-mysql.svcport:3306databases:uaa:name:uaapassword:\u0026lt;root_password\u0026gt; username: rootcc:name:cloud_controllerpassword:\u0026lt;root_password\u0026gt; username: rootbbs:name:diegopassword:\u0026lt;root_password\u0026gt; username: rootrouting_api:name:routing-apipassword:\u0026lt;root_password\u0026gt; username: rootpolicy_server:name:network_policypassword:\u0026lt;root_password\u0026gt; username: rootsilk_controller:name:network_connectivitypassword:\u0026lt;root_password\u0026gt; username: rootlocket:name:locketpassword:\u0026lt;root_password\u0026gt; username: rootcredhub:name:credhubpassword:\u0026lt;root_password\u0026gt; username: root","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/getting-started/kubernetes-deploy/","title":"Deploy on Kubernetes"},{"body":"The tools in directory dev/kube help developers inspect kube clusters and kubecf deployments.\nThe scripts and their uses are:\n klog.sh:\nRun after kubecf is deployed, this script pulls the kube logs from all containers in all kubecf pods, as well as kubecf log files in the containers, pod descriptions, events, and resources.\n kube-ready-state-check.sh:\nRun before kubecf is deployed, this script inspects the kube cluster for issues known to impede kubecf deployment.\n pod-status:\nRun during or after kubecf is deployed, this script shows a table of all pods in the deployment and their state. Options exists to restrict it to a specific namespace and to watch continously.\n  ","excerpt":"The tools in directory dev/kube help developers inspect kube clusters and kubecf deployments.\nThe …","ref":"https://kubecf.suse.dev/docs/reference/layout/kube/","title":"Inspection Helpers"},{"body":" Deployment and teardown For developing with KinD, start a local cluster by running the start target:\nbazel run //dev/kind:start And tear it down via:\nbazel run //dev/kind:delete","excerpt":" Deployment and teardown For developing with KinD, start a local cluster by running the start …","ref":"https://kubecf.suse.dev/docs/reference/layout/kind/","title":"Kind"},{"body":"The target defined in directory dev/kubecf is used to apply the rendered Helm template to a Kubernetes cluster.\nAttention: While any files matching the glob pattern *values.yaml and found in this directory are ignored by git, they are used by Bazel to render the SCF chart before applying to the cluster with kubectl.\n","excerpt":"The target defined in directory dev/kubecf is used to apply the rendered Helm template to a …","ref":"https://kubecf.suse.dev/docs/reference/layout/kubecf/","title":"KubeCF"},{"body":"The directory dev/linters contains tooling to statically check the sources of kubecf.\nThe following linters are available:\n shellcheck.sh:\nRuns shellcheck on all .sh files found in the entire checkout and reports any issues found.\n yamllint.sh:\nRuns yamllint on all .{yaml,yml} files found in the entire checkout and reports any issues found.\n helmlint.sh:\nRuns helm lint on the generated kubecf chart and repos any errors found.\n  ","excerpt":"The directory dev/linters contains tooling to statically check the sources of kubecf.\nThe following …","ref":"https://kubecf.suse.dev/docs/reference/layout/linters/","title":"Linters"},{"body":" Attention Minikube edits the kubernetes configuration file referenced by the environment variable KUBECONFIG, or ~/.kube/config.\nTo preserve the original configuration either make a backup of the relevant file, or change KUBECONFIG to a different path specific to the intended deployment.\nDeployment and teardown For developing with Minikube, start a local cluster by running the start target:\nbazel run //dev/minikube:start And tear it down again via:\nbazel run //dev/minikube:delete Managing system resources The following three environment variables are used by the start target to allocate the resources used by the deployed Minikube:\n   Variable Setting     VM_CPUS the number of CPUs Minikube will use.   VM_MEMORY the amount of RAM Minikube will be allowed to use.   VM_DISK_SIZE the disk size Minikube will be allowed to use.    E.g.:\nVM_CPUS=6 VM_MEMORY=$((1024 * 24)) VM_DISK_SIZE=180g bazel run //dev/minikube:start Specifying a different Kubernetes version Set the K8S_VERSION environment variable to override the default version.\nVM Drivers At the moment, only the VirtualBox and KVM2 drivers are working correctly. Set the VM_DRIVER environment variable to override the default. E.g. VM_DRIVER=kvm2.\nExtra minikube options It is possible to set extra minikube options (e.g. to set a docker registry mirror) via the environment variable MINIKUBE_EXTRA_OPTIONS. For example:\nexport MINIKUBE_EXTRA_OPTIONS=\u0026#34;--registry-mirror http://registry.mirror.example:5000/\u0026#34;","excerpt":"Attention Minikube edits the kubernetes configuration file referenced by the environment variable …","ref":"https://kubecf.suse.dev/docs/reference/layout/minikube/","title":"Minikube"},{"body":" The directory bosh/releases/pre_render_scripts contains scripts added to the quarks property.\nThe structure of this directory is expected to be \u0026lt;instance_group\u0026gt;/\u0026lt;job\u0026gt;/\u0026lt;type\u0026gt;/\u0026lt;script\u0026gt;, where \u0026lt;type\u0026gt; is one of\n jobs bpm ig_resolver  This type details the exact location of where the patch executes.\nBackground Pre render scripts are kubecf\u0026rsquo;s equivalent of its predecessors\u0026rsquo; (SCF v2) patches scripts. They, like them, enable developers and maintainers to apply general patches to the sources of a job (e.g. configuration templates, script sources, etc.) before that job was rendered and then executed.\nAt the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nMachinery The parent and sibling directories (bosh/releases and bosh/releases/generators) contain supporting code (bazel machinery) which will automatically convert the script files into the proper ops files for use by the CF operator, as part of the overall generation of the kubecf helm chart.\nIt is this machinery which depends on the \u0026lt;instance_group\u0026gt;/\u0026lt;job\u0026gt;/\u0026lt;type\u0026gt;/\u0026lt;script\u0026gt; structure noted above.\nAttention, Dangers All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\n","excerpt":"The directory bosh/releases/pre_render_scripts contains scripts added to the quarks property.\nThe …","ref":"https://kubecf.suse.dev/docs/reference/layout/patches/","title":"Pre-render scripts"},{"body":"The important directories of the KubeCF sources, and their contents are shown in the table below. Each directory entry links to the associated documentation, if we have any.\n   Directory Content     top Documentation entrypoint, License,    Main workspace definitions.   top/\u0026hellip;/README.md Directory-specific local documentation.   top/bosh/releases Support for runtime patches of a kubecf deployment.   top/doc Global documentation.   top/dev/cf_deployment/bump Tools to support updating the cf deployment    manifest used by kubecf.   top/dev/cf_cli Deploy cf cli into a helper pod from which to then    inspect the deployed Kubecf   top/dev/kube Tools to inspect kube clusters and kubecf deployments.   top/dev/linters Tools for statically checking the kubecf sources.   top/dev/minikube Targets to manage a local kubernetes cluster.    Minikube based.   top/dev/kind Targets to manage a local kubernetes cluster.    KinD based (Kube-in-Docker).   top/dev/kubecf Kubecf chart configuration, and targets for    local chart application.   top/deploy/helm/kubecf Templates and assets wrapping a CF deployment    manifest into a helm chart.   top/rules Supporting bazel definitions.   top/testing Bazel targets to run CF smoke and acceptance tests.    ","excerpt":"The important directories of the KubeCF sources, and their contents are shown in the table below. …","ref":"https://kubecf.suse.dev/docs/reference/layout/","title":"Project Layout"},{"body":"The smoke, brain, and acceptance tests can be run after Kubecf deployment has completed, via:\nbazel run //testing/smoke_tests bazel run //testing/brain_tests bazel run //testing/acceptance_tests The acceptance tests can be limited to specific suites of interest, as explained in the linked document.\n","excerpt":"The smoke, brain, and acceptance tests can be run after Kubecf deployment has completed, via:\nbazel …","ref":"https://kubecf.suse.dev/docs/reference/layout/testing/","title":"Testing"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/index.json","title":""},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  About KubeCF Cloud Foundry in Kubernetes.        KubeCF uses the Quarks operator The Quarks project extends Kubernetes to understand BOSH     KubeCF is designed to bring existing CF components in Kubernetes, leveraging existing BOSH releases.       Smooth transitioning from BOSH to Kubernetes native components      ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"https://kubecf.suse.dev/about/","title":"About KubeCF"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will …","ref":"https://kubecf.suse.dev/blog/","title":"Docsy Blog"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/community/","title":"Community"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_top.jpg); } }  KubeCF Learn More   Download   Cloud Foundry on Kubernetes\n          Cloud Foundry built for Kubernetes (formerly SUSE/scf v3 branch). It makes use of the Cloud Foundry Operator, which is incubating under Project Quarks.       CFAR KubeCF produces builds of CFAR which can be deployed to Kubernetes with Helm and managed by the cf-operator.\n   BOSH KubeCF already consumes your BOSH release and packages it for Kubernetes\n   Integration cf-operator extends Kubernetes to understand BOSH\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"https://kubecf.suse.dev/","title":"KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/search/","title":"Search Results"}]