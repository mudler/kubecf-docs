[{"body":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy and track CF Deployments, consuming directly also BOSH Releases, among Kubernetes Native components.\nWhere should I go next?  Getting Started: Get started with KubeCF Tutorials: Check out some tutorials!  ","excerpt":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy …","ref":"https://mudler.github.io/kubecf-docs/docs/overview/","title":"Overview"},{"body":" There is not an official release yet, check out our nightlies.\n Prerequisites  A Kubernetes cluster Presence of a default storage class (provisioner). For use with a diego-based kubecf (default), a node OS with XFS support.  For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   Installation KubeCF is packaged as an Helm chart.\nCurrently there isn\u0026rsquo;t any official release.\nNightly builds can be found on the KubeCF public s3 bucket.\nTry it out! ","excerpt":"There is not an official release yet, check out our nightlies.\n Prerequisites  A Kubernetes cluster …","ref":"https://mudler.github.io/kubecf-docs/docs/getting-started/","title":"Getting Started"},{"body":" Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The Image Modify Kubecf to Use the New Image  Integrating the Release in Kubecf  BPM Operation Files  Testing With Kubecf  Preparing the Release Image BOSH release authors who want to test their development code with the Quarks operator need to build a Docker image from their release. This can be done with fissile. Afterwards, upload the image to a cluster for testing it, e.g. with Kubecf.\nBuilding a Docker Image with Fissile Build the BOSH release first and convert it with fissile.\nTo generate a docker image from the BOSH release, you should use the following subcommand:\nfissile build release-image For more information on how to use the command, please refer to the related documentation. For a real example, see build.sh.\nUploading The Image Depending on your cluster, you will need a way to get the locally built image into the Kubernetes registry.\nWith minikube you can build directly on minikube\u0026rsquo;s Docker. Switch to that docker daemon by running eval $(minikube docker-env), before you build the image with fissile.\nWith kind, you need to use kind load docker-image after building the image, to make it available, i.e.:\nkind load docker-image docker.io/org/nats:0.1-dev Modify Kubecf to Use the New Image Add an operations file to Kubernetes with the new image location. The example below uses NATS as the example for a BOSH release.\nkubectlapply-f-\u0026lt;\u0026lt;EOF---apiVersion:v1kind:ConfigMapmetadata:name:nats-devdata:ops:|-type:replacepath:/releases/name=nats?value:name:natsurl:docker.io/org/natsversion:0.1-devsha1:~EOF Then, when running helm install kubecf, refer to that image:\nhelm install ... --set \u0026#39;operations.custom={nats-dev}\u0026#39; Note: You can also unpack the helm release and modify it directly. There is no need to zip the release again, as helm install scf/ is able to install the unpacked release.\nNote further that the above is an example of how to use the first kind of customization feature noted in the main README.\nIntegrating the Release in Kubecf With Quarks and Kubecf, BOSH releases can largely be used just the same as with a BOSH director. There are a few things Quarks offers, however, to make the adaptation to the Kubernetes environment easier.\nBPM BPM configurations for jobs are parsed from a rendered bpm.yml, as usual. But if need be, it is also possible to override the BPM configuration in the deployment manifest in the quarks field. See the bpm documentation for details on how to configure BPM.\nExample:\ninstance_groups:-name:natsinstances:2jobs:-name:natsproperties:quarks:bpm:processes:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Note: The next section on ops files explains how this can be applied without the need to modify the original deployment manifest using ops files.\nOperation Files ops files can be used to modify arbitrary parts of the deployment manifest before it is applied. To do so, create a file in the directory deploy/helm/scf/assets/operations/instance_groups and it will be automagically applied during installation, courtesy of the bazel machinery.\nThe ops file for the example above could look like this:\n-type:replacepath:/instance_groups/name=nats/jobs/name=nats/properties/quarks?/bpm/processesvalue:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Testing With Kubecf After upload and integration, it is possible to build and deploy Kubecf according to any of the recipes listed by the main README.\n","excerpt":"Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The …","ref":"https://mudler.github.io/kubecf-docs/docs/tutorials/bosh-integration/","title":"Bosh releases integration"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf locally using:\n Minikube to manage a local Kubernetes cluster. A cf-operator pinned with Bazel. Kubecf built and deployed from the sources in the current checkout.  Minikube Minikube is one of several projects enabling the deployment, management and tear-down of a local Kubernetes cluster.\nThe Kubecf Bazel workspace contains targets to deploy and/or tear-down a Minikube-based cluster. Using these has the advantage of using a specific version of Minikube. On the other side, the reduced variability of the development environment is a disadvantage as well, possibly allowing portability issues to slide through.\n   Operation Command     Deployment bazel run //dev/minikube:start   Tear-down bazel run //dev/minikube:delete    Attention, Dangers Minikube edits the Kubernetes configuration file referenced by the environment variable KUBECONFIG, or ~/.kube/config.\nTo preserve the original configuration either make a backup of the relevant file, or change KUBECONFIG to a different path specific to the intended deployment.\nAdvanced configuration The local Minikube Documentation explains the various environment variables which can be used to configure the resources used by the cluster (CPUs, memory, disk size, etc.) in detail.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same kube cluster Kubecf will be deployed to.\nDeployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down cf-operator:\n   Operation Command     Deployment bazel run //dev/cf_operator:apply   Tear-down bazel run //dev/cf_operator:delete    Kubecf With all the prequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nSystem domain The main configuration to set for kubecf is its system domain. For the Minikube foundation we have to specify it as:\necho \u0026#34;system_domain: $(minikube ip).xip.io\u0026#34; \\  \u0026gt; \u0026#34;$(bazel info workspace)/dev/kubecf/system_domain_values.yaml\u0026#34; Deployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down kubecf from the sources:\n   Operation Command     Deployment bazel run //dev/kubecf:apply   Tear-down bazel run //dev/kubecf:delete    In this default deployment kubecf is launched without Ingress, and uses the Diego scheduler.\nAccess Accessing the cluster from outside of the minikube VM requires ingress to be set up correctly.\nTo access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.$(minikube ip).xip.io\u0026#34; # Copy the admin cluster password. acp=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${acp}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, add a file matching the pattern *values.yaml to the directory dev/kubecf and containing\nfeatures:eirini:enabled:true before deploying kubecf.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/kubecf-scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; \\  --set \u0026#34;controller.service.externalIPs={$(minikube ip)}\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nThe last flag in the command above assigns the external IP of the cluster to the Ingress Controller service.\nConfigure kubecf Place a file matching the pattern *values.yaml into the directory dev/kubecf and containing\nfeatures:ingress:enabled:true","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://mudler.github.io/kubecf-docs/docs/tutorials/deploy-minikube/","title":"Deploy KubeCF in Minikube"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/concepts/","title":"Concepts"},{"body":" The CF-Operator is a Cloud Foundry Incubating Project.\n ","excerpt":" The CF-Operator is a Cloud Foundry Incubating Project.\n ","ref":"https://mudler.github.io/kubecf-docs/docs/concepts/operator/","title":"Cloud Foundry Operator"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tasks/","title":"Core Tasks"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tasks/secrets/","title":"Secret rotation KubeCF"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tasks/deploy/","title":"Deploy KubeCF"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tasks/upgrade/","title":"Upgrading KubeCF deployments"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tasks/bosh/","title":"Test BOSH releases"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/docs/tutorials/","title":"Tutorials"},{"body":"Here are grouped all the docs which refers to internals or specific code areas of KubeCF\n","excerpt":"Here are grouped all the docs which refers to internals or specific code areas of KubeCF","ref":"https://mudler.github.io/kubecf-docs/docs/reference/","title":"Reference"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nIt provides a basic overview of various aspects of the project below, and uses these overviews as the launching points to other documents which go deeper into the details of each aspect.\n Table of Contents (Aspects)  Deployment Pull Requests Source Organization Docker Images Linting Patching BOSH Development Workflow  Deployment Kubecf is built on top of a number of technologies, namely Kubernetes, Helm (charts), and the cf-operator.\nFor all these we have multiple choices for installing them, and various interactions between the choices influence the details of the commands to use.\nInstead of trying to document all the possibilities and all their interactions at once, supporting documents will describe specific combinations of choices in detail, from the bottom up.\n   Document Description     Local Minikube Minikube/Bazel + Operator/Bazel + Kubecf/Bazel   General Kube Any Kube + Operator/Helm + Kubecf/Helm    Pull Requests The general work flow for pull requests contributing bug fixes, features, etc. is:\n Branch or Fork the suse/kubecf repository, depending on permissions.\n Implement the bug fix, feature, etc. on that branch/fork.\n Submit a pull request based on the branch/fork through the github web interface, against the master branch.\n Developers will review the content of the pull request, asking questions, requesting changes, and generally discussing the submission with the submitter and among themselves.\n After all issues with the request are resolved, and CI has passed, a developer will merge it into master.\n Note that it may be necessary to rebase the branch/fork to resolve any conflicts due to other PRs getting merging while the PR is under discussion.\nSuch a rebase will be a change request from the developers to the contributor, on the assumption that the contributor is best suited to resolving the conflicts.\n  Docker Images The docker images used by kubecf to run jobs in container use a moderately complex naming scheme.\nThis scheme is explained in a separate document: The Naming Of Docker Images in kubecf.\nLinting Currently, 3 linters are available:\n dev/linters/shellcheck.sh dev/linters/yamllint.sh dev/linters/helmlint.sh  Invoke these linters as\ndev/linters/shellcheck.sh dev/linters/yamllint.sh dev/linters/helmlint.sh to run shellcheck on all .sh files found in the entire checkout, or yamllint on all .yaml or .yml files respectively, and report any issues found. The last option runs helm lint (without --strict) on the generated helm chart.\nPatching Background The main goal of the CF operator is to take a BOSH deployment manifest, deploy it, and have it run as-is.\nNaturally, in practice, this goal is not quite reached yet, requiring patching of the deployment manifest in question, and/or the involved releases, at various points of the deployment process. The reason behind a patch is generally fixing a problem, whether it be from the translation into the kube environment, an issue with an underlying component, or something else.\nThen, there are features, given the user of the helm chart wrapped around the deployment manifest the ability to easily toggle various preset configurations, for example the use of eirini instead of diego as the application scheduler.\nFeatures A feature of kubecf is usually implemented using a combination of Helm templating and BOSH ops files.\nThe helm templating is used to translate the properties in the chart\u0026rsquo;s values.yaml to the actual actions to take, by including/excluding chart elements, often the BOSH ops files containing the structured patches modifying the deployment itself (changing properties, adding/removing releases, (de)activating jobs, etc.)\nThe helm templating is applied when the kubecf chart is deployed.\nThe ops files are then applied by the operator, transforming the base manifest from the chart into the final manifest to deploy.\nCustomization Kubecf provides two mechanisms for customization during development (and maybe by operators ?):\n The property .Values.operations.custom of the chart is a list of names for kube configmaps containing the texts of the ops files to apply beyond the ops files from the chart itself.\nNote that we are talking here about a yaml structure whose data.ops property is a text block holding the yaml structure of an ops file.\nThere is no tooling to help the writer with the ensuing quoting hell.\nNote further that the resulting config maps have to be applied, i.e. uploaded into the kube cluster before deploying the kubecf helm chart with its modified values.yaml.\nFor example, kubectl apply the object below\n---apiVersion:v1kind:ConfigMapmetadata:name:configmap_namedata:ops:|-some_random_ops and then use\noperations:custom:-configmap_name  in the values.yaml (or an equivalent --set option) as part of a kubecf deployment to include that ops file in the deployment.\nThe BOSH Development Workflow is an example of its use.\n The second mechanism allows the specification of any custom BOSH property for any instancegroup and job therein.\nJust specifying\nproperties:instance-group-name:job-name:some-property:some-value  in the values.yaml for the kubecf chart causes the chart to generate and use an ops file which applies the assignment of some-value to some-property to the specified instance group and job during deployment.\nAn example of its use in Kubecf is limiting the set of test suites executed by the CF acceptance tests.\nBoth forms of customization assume a great deal of familiarity on the part of the developer and/or operator with the BOSH releases, instance groups and jobs underlying the CF deployment manifest, i.e. which properties exist, what changes to them mean and how they affect the system.\nPatches In SCF v2, the predecessor to kubecf, the patches scripts enabled developers and maintainers to apply general patches to the sources of a job (i.e. configuration templates, script sources, etc.) before that job was rendered and then executed. At the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nPre render scripts are the equivalent feature of the CF operator.\nKubecf makes use of this feature to fix a number of issues in the deployment. The relevant patch scripts are found under the directory bosh/releases/pre_render_scripts.\nWhen following the directory structure explained by the README, the bazel machinery for generating the kubecf helm chart will automatically convert these scripts into the proper ops files for use by the CF operator.\nAttention All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\n","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://mudler.github.io/kubecf-docs/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":" KubeCF is under active development. Note there might be discrepancies between the docs and latest releases.\n ","excerpt":" KubeCF is under active development. Note there might be discrepancies between the docs and latest …","ref":"https://mudler.github.io/kubecf-docs/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/blog/news/","title":"News About KubeCF"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/blog/releases/","title":"New Releases"},{"body":" This is a typical blog post that includes images.\nThe front matter specifies the date of the blog post, its title, a short description that will be displayed on the blog landing page, and its author.\nIncluding images Here\u0026rsquo;s an image (featured-sunset-get.png) that includes a byline and a caption.\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\n  The front matter of this post specifies properties to be assigned to all image resources:\nresources: - src: \u0026quot;**.{png,jpg}\u0026quot; title: \u0026quot;Image #:counter\u0026quot; params: byline: \u0026quot;Photo: Riona MacNamara / CC-BY-CA\u0026quot;  To include the image in a page, specify its details like this:\n Fetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\n   The image will be rendered at the size and byline specified in the front matter.\n","excerpt":"This is a typical blog post that includes images.\nThe front matter specifies the date of the blog …","ref":"https://mudler.github.io/kubecf-docs/blog/2018/10/06/easy-documentation-with-docsy/","title":"Easy documentation with Docsy"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/blog/2018/01/04/another-great-release/","title":"Another Great Release"},{"body":" Bump The scripts in directory dev/cf_deployment/bump are used when bumping the version of the cf-deployment manifest.\nRequirements  yq  Scripts  set_release_urls.sh:\nTo be run after updating the cf-deployment version to use and regenerating assets/cf-deployment.yml, this script regenerates the assets/operations/set_release_urls.yaml ops file which replaces BOSH release locations with references to the equivalent docker images.\n  ","excerpt":" Bump The scripts in directory dev/cf_deployment/bump are used when bumping the version of the …","ref":"https://mudler.github.io/kubecf-docs/docs/reference/layout/bump/","title":"CF Bump scripts"},{"body":" cf-cli The cf_cli.sh script contained in directory dev/cf_cli is a temporary helper script that creates a pod with the cf-cli configured and ready to talk to the Kubecf in the cluster.\n","excerpt":"cf-cli The cf_cli.sh script contained in directory dev/cf_cli is a temporary helper script that …","ref":"https://mudler.github.io/kubecf-docs/docs/reference/layout/cfcli/","title":"CF CLI script"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf using:\n A generic kubernetes cluster. A released cf-operator helm chart. A released kubecf helm chart.  Kubernetes In contrast to other recipes, we are not set on using a local cluster. Any Kubernetes cluster will do, assuming that the following requirements are met:\n Presence of a default storage class (provisioner).\n For use with a diego-based kubecf (default), a node OS with XFS support.\n For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   This can be any of, but not restricted to:\n GKE (Notes) AKS EKS  Note that how to deploy and tear-down such a cluster is outside of the scope of this recipe.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same Kubernetes cluster that Kubecf will be deployed to.\nHere we are not using development-specific dependencies like bazel, but only generic tools, i.e. kubectl and helm.\nInstalling and configuring Helm is the same regardless of the chosen foundation, and assuming that the cluster does not come with Helm Tiller pre-installed.\nDeployment and Tear-down helm install --name cf-operator \\  --namespace cfo \\  --set \u0026#34;global.operator.watchNamespace=kubecf\u0026#34; \\  https://s3.amazonaws.com/cf-operators/helm-charts/cf-operator-v0.4.2-147.gb88e4296.tgz In the example above, version 0.4.1 of the operator was used. Look into the cf_operator section of the top-level def.bzl file to find the version of the operator validated against the current kubecf master.\nNote: \u0026gt; The above helm install will generate many controllers spread over multiple pods inside the cfo namespace. \u0026gt; Most of these controllers run inside the cf-operator pod. \u0026gt; \u0026gt; The global.operator.watchNamespace=kubecf path tells the controllers to watch for CRD´s instances into the kubecf namespace. \u0026gt; \u0026gt; The cf-operator helm chart will generate the kubecf namespace during installation, and eventually one of the controllers will use a webhook to label this namespace with the cf-operator-ns key. \u0026gt; \u0026gt; If the kubecf namespace is deleted, but the operators are still running, they will no longer know which namespace to watch. This can lead to problems, so make sure you also delete the pods inside the cfo namespace, after deleting the kubecf namespace.\nNote how the namespace the operator is installed into (cfo) differs from the namespace the operator is watching for deployments (kubecf).\nThis form of deployment enables restarting the operator because it is not affected by webhooks. It further enables the deletion of the Kubecf deployment namespace to start from scratch, without redeploying the operator itself.\nTear-down is done with a standard helm delete ... command.\nKubecf With all the prerequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nThis again uses helm and a released helm chart.\nDeployment and Tear-down helm install --name kubecf \\  --namespace kubecf \\  https://scf-v3.s3.amazonaws.com/scf-3.0.0-82165ef3.tgz \\  --set \u0026#34;system_domain=kubecf.suse.dev\u0026#34; In this default deployment, kubecf is launched without Ingress, and it uses the Diego scheduler.\nTear-down is done with a standard helm delete ... command.\nAccess To access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.\u0026lt;domain\u0026gt;\u0026#34; # Copy the admin cluster password. admin_pass=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${admin_pass}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, use the option --set features.eirini.enabled=true when deploying kubecf from its chart.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/kubecf-scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nConfigure kubecf Use the Helm option --set features.ingress.enabled=true when deploying kubecf.\nExternal Database By default, kubecf includes a single-availability database provided by the cf-mysql-release. Kubecf also exposes a way to use an external database via the Helm property features.external_database. Check the values.yaml for more details.\nFor local development with an external database, the bazel run //dev/external_database:deploy_mysql command will bring a mysql database up and running ready to be consumed by kubecf.\nAn example for the additional values to be provided to //dev/kubecf:apply:\nfeatures:external_database:enabled:truetype:mysqlhost:kubecf-mysql.kubecf-mysql.svcport:3306databases:uaa:name:uaapassword:\u0026lt;root_password\u0026gt; username: rootcc:name:cloud_controllerpassword:\u0026lt;root_password\u0026gt; username: rootbbs:name:diegopassword:\u0026lt;root_password\u0026gt; username: rootrouting_api:name:routing-apipassword:\u0026lt;root_password\u0026gt; username: rootpolicy_server:name:network_policypassword:\u0026lt;root_password\u0026gt; username: rootsilk_controller:name:network_connectivitypassword:\u0026lt;root_password\u0026gt; username: rootlocket:name:locketpassword:\u0026lt;root_password\u0026gt; username: rootcredhub:name:credhubpassword:\u0026lt;root_password\u0026gt; username: root","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://mudler.github.io/kubecf-docs/docs/getting-started/kubernetes-deploy/","title":"Deploy on Kubernetes"},{"body":" The directory bosh/releases/pre_render_scripts contains scripts added to the quarks property.\nThe structure of this directory is expected to be \u0026lt;instance_group\u0026gt;/\u0026lt;job\u0026gt;/\u0026lt;type\u0026gt;/\u0026lt;script\u0026gt;, where \u0026lt;type\u0026gt; is one of\n jobs bpm ig_resolver  This type details the exact location of where the patch executes.\nBackground Pre render scripts are kubecf\u0026rsquo;s equivalent of its predecessors\u0026rsquo; (SCF v2) patches scripts. They, like them, enable developers and maintainers to apply general patches to the sources of a job (e.g. configuration templates, script sources, etc.) before that job was rendered and then executed.\nAt the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nMachinery The parent and sibling directories (bosh/releases and bosh/releases/generators) contain supporting code (bazel machinery) which will automatically convert the script files into the proper ops files for use by the CF operator, as part of the overall generation of the kubecf helm chart.\nIt is this machinery which depends on the \u0026lt;instance_group\u0026gt;/\u0026lt;job\u0026gt;/\u0026lt;type\u0026gt;/\u0026lt;script\u0026gt; structure noted above.\nAttention, Dangers All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\n","excerpt":"The directory bosh/releases/pre_render_scripts contains scripts added to the quarks property.\nThe …","ref":"https://mudler.github.io/kubecf-docs/docs/reference/layout/patches/","title":"Pre-render scripts"},{"body":" Source Organization The important directories of the kubecf sources, and their contents are shown in the table below. Each directory entry links to the associated documentation, if we have any.\n   Directory Content     top Documentation entrypoint, License,    Main workspace definitions.   top/\u0026hellip;/README.md Directory-specific local documentation.   top/bosh/releases Support for runtime patches of a kubecf deployment.   top/doc Global documentation.   top/dev/cf_deployment/bump Tools to support updating the cf deployment    manifest used by kubecf.   top/dev/cf_cli Deploy cf cli into a helper pod from which to then    inspect the deployed Kubecf   top/dev/kube Tools to inspect kube clusters and kubecf deployments.   top/dev/linters Tools for statically checking the kubecf sources.   top/dev/minikube Targets to manage a local kubernetes cluster.    Minikube based.   top/dev/kind Targets to manage a local kubernetes cluster.    KinD based (Kube-in-Docker).   top/dev/kubecf Kubecf chart configuration, and targets for    local chart application.   top/deploy/helm/kubecf Templates and assets wrapping a CF deployment    manifest into a helm chart.   top/rules Supporting bazel definitions.   top/testing Bazel targets to run CF smoke and acceptance tests.    ","excerpt":"Source Organization The important directories of the kubecf sources, and their contents are shown in …","ref":"https://mudler.github.io/kubecf-docs/docs/reference/layout/","title":"Project Layout"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/index.json","title":""},{"body":"  #td-cover-block-0 { background-image: url(/kubecf-docs/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/kubecf-docs/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  About KubeCF Cloud Foundry in Kubernetes.        KubeCF uses the Quarks operator The Quarks project extends Kubernetes to understand BOSH     KubeCF is designed to bring existing CF components in Kubernetes, leveraging existing BOSH releases.       Smooth transitioning from BOSH to Kubernetes native components      ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"https://mudler.github.io/kubecf-docs/about/","title":"About KubeCF"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will …","ref":"https://mudler.github.io/kubecf-docs/blog/","title":"Docsy Blog"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/community/","title":"Community"},{"body":"  #td-cover-block-0 { background-image: url(/kubecf-docs/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/kubecf-docs/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_top.jpg); } }  KubeCF Learn More   Download   Cloud Foundry on Kubernetes\n          Cloud Foundry built for Kubernetes (formerly SUSE/scf v3 branch). It makes use of the Cloud Foundry Operator, which is incubating under Project Quarks.       CFAR KubeCF produces builds of CFAR which can be deployed to Kubernetes with Helm and managed by the cf-operator.\n   BOSH KubeCF already consumes your BOSH release and packages it for Kubernetes\n   Integration cf-operator extends Kubernetes to understand BOSH\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"https://mudler.github.io/kubecf-docs/","title":"KubeCF"},{"body":"","excerpt":"","ref":"https://mudler.github.io/kubecf-docs/search/","title":"Search Results"}]